{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyPiYrLC8l/8rHHE53eXNnVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratnesh003/HCLTech-Tasks/blob/main/Silver%20Badge%20Assignments/Assignment%201%20/HCLTech_ML_Assignment_2_Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1"
      ],
      "metadata": {
        "id": "QRpRxMZCALbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use any other open-source model in place of openai and do the summarization of the pdf document using Lang chain and the concept of RAG. use chromadb vector database for this"
      ],
      "metadata": {
        "id": "IDHcZJqpATXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies"
      ],
      "metadata": {
        "id": "8xS8YUb8HiUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  requests \\\n",
        "  opentelemetry-api \\\n",
        "  opentelemetry-sdk \\\n",
        "  opentelemetry-proto \\\n",
        "  opentelemetry-exporter-otlp-proto-common \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  langchain-huggingface \\\n",
        "  chromadb \\\n",
        "  pypdf \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D74KYu_25C_N",
        "outputId": "dd153023-7b6b-4725-d3b4-ded3a71f7d4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.12/dist-packages (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.12/dist-packages (1.37.0)\n",
            "Collecting opentelemetry-proto\n",
            "  Downloading opentelemetry_proto-1.39.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api) (4.15.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk) (0.58b0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto) (5.29.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "INFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.35.3)\n",
            "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "  Downloading langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.3-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.37.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.75.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.19.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api) (3.23.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-sdk\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.39.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.39.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is looking at multiple versions of opentelemetry-exporter-otlp-proto-grpc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.37.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: pip is still looking at multiple versions of opentelemetry-exporter-otlp-proto-grpc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.31.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.31.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.25.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.20.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.19.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.19.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.18.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.18.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.17.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.16.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.15.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.14.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.13.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.12.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.11.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.10.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-sdk\n",
            "  Downloading opentelemetry_sdk-1.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.9.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.9.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.8.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.7.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.7.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.6.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.6.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.6.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.5.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.4.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.3.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.3.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.3.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.3.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.2.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "INFO: pip is still looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading chromadb-1.2.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.2.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.1.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-1.0.21-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading chromadb-1.0.20-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-1.0.19-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-1.0.18-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-1.0.17-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "  Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-7.4.0-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.12-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.60b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.11-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.10-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.7-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-1.0.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.118.2)\n",
            "  Downloading chromadb-0.6.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.6.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
            "  Downloading tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.21-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.20-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.17-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.16-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.12-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.11-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.10-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.9-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting numpy>=1.26.2 (from langchain-community)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.5.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting chroma-hnswlib==0.7.5 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma-hnswlib-0.7.3.tar.gz (31 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "  Downloading chromadb-0.5.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.23-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.21-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.20-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.19-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.18-py3-none-any.whl.metadata (7.4 kB)\n",
            "  Downloading chromadb-0.4.17-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.16-py3-none-any.whl.metadata (7.3 kB)\n",
            "  Downloading chromadb-0.4.15-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Downloading chromadb-0.4.14-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi>=0.95.2->chromadb) (0.48.0)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Downloading opentelemetry_proto-1.39.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.39.1-py3-none-any.whl (18 kB)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading chromadb-0.4.14-py3-none-any.whl (448 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.4.2-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.2/328.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-7.4.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.1/166.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pulsar_client-3.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: chroma-hnswlib, pypika\n",
            "  Building wheel for chroma-hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chroma-hnswlib: filename=chroma_hnswlib-0.7.3-cp312-cp312-linux_x86_64.whl size=2530620 sha256=d5a9c6d57afd7054d12359aabec9dcab13d819554ebff811dc0260405fa38a4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/14/b5/68c4f2e056600c0348a94efba92dc975686ab72b714e0ca3d6\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=e669a47f23c1afd07beb11a8d489d15a6535c85c9f35d7f229028939f5ce8f64\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built chroma-hnswlib pypika\n",
            "Installing collected packages: pypika, uvloop, requests, pypdf, pulsar-client, opentelemetry-proto, mypy-extensions, marshmallow, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, coloredlogs, onnxruntime, dataclasses-json, chromadb, bitsandbytes, langchain-huggingface, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 bitsandbytes-0.49.0 chroma-hnswlib-0.7.3 chromadb-0.4.14 coloredlogs-15.0.1 dataclasses-json-0.6.7 httptools-0.7.1 humanfriendly-10.0 langchain-community-0.3.31 langchain-huggingface-0.3.1 marshmallow-3.26.1 mypy-extensions-1.1.0 onnxruntime-1.23.2 opentelemetry-exporter-otlp-proto-common-1.39.1 opentelemetry-proto-1.39.1 posthog-7.4.0 pulsar-client-3.8.0 pypdf-6.4.2 pypika-0.48.9 requests-2.32.5 typing-inspect-0.9.0 uvloop-0.22.1 watchfiles-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the PDF"
      ],
      "metadata": {
        "id": "LXnyNhtnPIhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Prompt user to upload a PDF\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name (assumes one PDF)\n",
        "pdf_path = next(iter(uploaded))\n",
        "\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded PDF: {pdf_path}\")\n",
        "print(f\"Total pages loaded: {len(documents)}\")"
      ],
      "metadata": {
        "id": "Cp1UfmSk5Ffa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "64afae61-0966-466d-c408-076e2292acdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecdb4cb6-8728-40c2-9d5a-ae7066c04b2b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecdb4cb6-8728-40c2-9d5a-ae7066c04b2b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving azure_databricks_cookbook.pdf to azure_databricks_cookbook.pdf\n",
            "Loaded PDF: azure_databricks_cookbook.pdf\n",
            "Total pages loaded: 452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create the chunks"
      ],
      "metadata": {
        "id": "6Sw1mXuRPSpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,          # Large enough for semantic coherence\n",
        "    chunk_overlap=300,        # Prevents abrupt cut-offs\n",
        "    separators=[\n",
        "        \"\\n\\n\",               # Paragraphs\n",
        "        \"\\n\",                 # Lines\n",
        "        \".\",                  # Sentences\n",
        "        \" \",                  # Words\n",
        "        \"\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total chunks created: {len(split_docs)}\")\n"
      ],
      "metadata": {
        "id": "Ftg06vun57lO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e462bca1-2396-4387-9dff-1a23f7e3767d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the embedding model"
      ],
      "metadata": {
        "id": "b7EzIyK4PXUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "CKS4udOZ59yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the chromaDB"
      ],
      "metadata": {
        "id": "ejhxhPX3Pd8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"/content/chroma_db\"\n",
        ")\n",
        "\n",
        "vectorstore.persist()\n"
      ],
      "metadata": {
        "id": "fAqw6D8K5_yM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033f4551-c5ce-432b-b148-e6613cf7c83e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
            "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
            "/tmp/ipython-input-1706748245.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Mistral model"
      ],
      "metadata": {
        "id": "u0Bgao7NPimJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "v5pfAPe16B1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the text generation pipeline"
      ],
      "metadata": {
        "id": "zbgQePQxPqdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
      ],
      "metadata": {
        "id": "-0IRAAsJ6Ecn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de95ae0-1ec3-4793-ec60-6d50123080d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3763298684.py:3: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the retrieval configurations"
      ],
      "metadata": {
        "id": "AJft91KSP04u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 10}\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "e6LMadph6GeB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Query and prompt for the generation"
      ],
      "metadata": {
        "id": "yw8AGppYP-Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask user for a custom query\n",
        "query = input(\n",
        "    \"Enter your question about the uploaded PDF:\\n\"\n",
        ").strip()\n",
        "\n",
        "if not query:\n",
        "    raise ValueError(\"Query cannot be empty.\")\n",
        "\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a senior data engineer.\n",
        "\n",
        "Using the context below, answer the question clearly and technically.\n",
        "Do NOT repeat the context.\n",
        "Do NOT include headings.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cVsdDj-O6In_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f91752-916a-491c-b3bc-b3276b836b70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question about the uploaded PDF:\n",
            "Explain in detail which Azure Databricks features should be used to ingest very high-volume real-time data sources. Provide a descriptive, technical explanation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-426534451.py:9: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query)\n",
            "ERROR:chromadb.telemetry.posthog:Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamming the text generation"
      ],
      "metadata": {
        "id": "niLqfj_0QKB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "MAX_INPUT_TOKENS = 8192\n",
        "\n",
        "# Create streamer\n",
        "streamer = TextIteratorStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Tokenize input WITH attention mask\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_INPUT_TOKENS\n",
        ").to(model.device)\n",
        "\n",
        "# Background generation\n",
        "thread = Thread(\n",
        "    target=model.generate,\n",
        "    kwargs={\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"do_sample\": True,          # REQUIRED for temperature\n",
        "        \"temperature\": 0.3,\n",
        "        \"repetition_penalty\": 1.1,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id\n",
        "    }\n",
        ")\n",
        "\n",
        "thread.start()\n",
        "\n",
        "# Stream answer tokens only\n",
        "print(\"\\nAnswer:\\n\")\n",
        "for token in streamer:\n",
        "    print(token, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "wrP-q5PW6LD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c00118-f7db-4d69-a89d-3c0e4c036c6b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Answer:\n",
            "\n",
            "To ingest very high-volume real-time data sources in Azure Databricks, the following features can be utilized:\n",
            "\n",
            "1. Azure Event Hubs or Apache Kafka: These are popular streaming platforms for handling massive volumes of real-time data. They offer scalability, reliability, and fault tolerance. Azure Event Hubs is a fully managed event ingestion service, while Apache Kafka is an open-source distributed streaming platform. Both can be integrated with Azure Databricks using Structured Streaming, allowing for real-time data processing.\n",
            "\n",
            "2. Structured Streaming: This is a feature of Apache Spark that enables real-time stream processing. It allows data to be read continuously from the source and processed in micro-batches. Structured Streaming supports various input and output formats, including CSV, Parquet, and JSON. It also supports windowing, stateful processing, and watermarks, making it suitable for handling complex real-time data processing tasks.\n",
            "\n",
            "3. Delta Lake: Delta Lake is an open-source, ACID-compliant, transactional data lake built on top of Apache Spark. It offers the benefits of both relational databases and data lakes. Delta Lake supports structured data, schema evolution, and ACID transactions. It also provides a unified API for batch and streaming data, making it an ideal choice for storing and processing real-time data. Additionally, Delta Lake includes Delta Engine, which optimizes query performance by rewriting queries to take advantage of Delta Lake's features.\n",
            "\n",
            "4. Autoscale Clusters: Azure Databricks offers autoscaling capabilities, which allow clusters to automatically adjust based on workload demand. This is particularly useful when dealing with high-volume real-time data sources, as it ensures that enough compute resources are always available to handle the incoming data.\n",
            "\n",
            "5. Personal Access Tokens (PAT): For added security, it's recommended to use personal access tokens instead of signing in directly to Azure Databricks. A PAT is a long-lived token that grants access to specific scopes within an Azure Databricks workspace. By limiting the scope of the token, you can reduce the risk of unauthorized access to sensitive data.\n",
            "\n",
            "Overall, using Azure Event Hubs or Apache Kafka as the data source, Structured Streaming for real-time processing, Delta Lake for data storage, aut"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing the references used for answer generation"
      ],
      "metadata": {
        "id": "7n_EIoUiQOzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nReferences used from the document:\\n\")\n",
        "\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    source = doc.metadata.get(\"source\", \"Unknown source\")\n",
        "    page = doc.metadata.get(\"page\", \"Unknown page\")\n",
        "    content = doc.page_content\n",
        "    print(f\"[{i}] Source: {source}, Page: {page} \\n Content: \\n{content}\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "iVG7Ra15Lw4R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70179643-3584-4438-9957-e757a3d39fd9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "References used from the document:\n",
            "\n",
            "[1] Source: azure_databricks_cookbook.pdf, Page: 13 \n",
            " Content: \n",
            "Preface\n",
            "Azure Databricks provides the latest and older versions of Apache Spark and allows you to \n",
            "integrate with various Azure resources for orchestrating, deploying, and monitoring your \n",
            "big data solution. This book shows you how you can ingest and transform data coming \n",
            "from various sources and formats and build a modern data warehouse solution that meets \n",
            "the demands of near real-time data requirements in the data warehouse.\n",
            "Y ou will begin with how to spin up an Azure Databricks service and what cluster options \n",
            "are available. Y ou will gain knowledge of how to process data from various files formats \n",
            "and sources, including Kafka, Event Hub, Azure SQL Databases, Azure Synapse Analytics, \n",
            "and Cosmos DB. Once you know how to read and write data from and to various sources, \n",
            "you will be building end-to-end big data solutions using large datasets and streaming data. \n",
            "Once the big data solution has been built, you will learn how to deploy notebooks to \n",
            "various environments such as UAT and production. Later on, you will learn security \n",
            "aspects associated with data isolation, where you will learn how to restrict access to the\n",
            "\n",
            "\n",
            "\n",
            "[2] Source: azure_databricks_cookbook.pdf, Page: 149 \n",
            " Content: \n",
            "4\n",
            "Working with \n",
            "Streaming Data\n",
            "As data ingestion pipelines evolve and change, we see a lot of streaming sources, such \n",
            "as Azure Event Hubs and Apache Kafka, being used as sources or sinks as part of data \n",
            "pipeline applications. Streaming data such as temperature sensor data and vehicle \n",
            "sensor data has become common these days. We need to build our data pipeline \n",
            "application in such a way that we can process streaming data in real time and at scale. \n",
            "Azure Databricks provides a great set of APIs, including Spark Structured Streaming, \n",
            "to process these events in real time. We can store the streaming data in various sinks, \n",
            "including Databricks File System (DBFS), in various file formats, in various streaming \n",
            "systems, such as Event Hubs and Kafka, and in databases such as Azure SQL databases, \n",
            "Azure Synapse Dedicated SQL pools, or in NoSQL databases such as Azure Cosmos. In \n",
            "this chapter, we will learn how to read data from various streaming sources using Azure \n",
            "Databricks Structured Streaming APIs and write the events to Delta tables, DBFS in \n",
            "Parquet format, and learn some internals on how checkpoints and triggers are used and\n",
            "\n",
            "\n",
            "\n",
            "[3] Source: azure_databricks_cookbook.pdf, Page: 14 \n",
            " Content: \n",
            "notebooks and explains how to integrate Azure Databricks with Log Analytics for \n",
            "telemetry.\n",
            "Chapter 6, Exploring Delta Lake in Azure Databricks, explains how to use Delta for batch \n",
            "and streaming data in Azure Databricks. Y ou will also understand how Delta Engine will \n",
            "assist in making your queries run faster.\n",
            "Chapter 7, Implementing Near-Real-Time Analytics and Building a Modern Data \n",
            "Warehouse, explains how to implement an end-to-end big data solution where you read \n",
            "data from streaming sources such as Event Hub for Kafka, as well as from batch sources \n",
            "such as ADLS Gen-2, and perform various data transformations on data and later store \n",
            "the data in destinations such as Azure Cosmos DB, Azure Synapse Analytics, and Delta \n",
            "Lake. Y ou will build a modern data warehouse and orchestrate the end-to-end pipeline \n",
            "using Azure Data Factory. Y ou will be performing near real-time analytics using notebook \n",
            "visualizations and Power BI.\n",
            "\n",
            "\n",
            "\n",
            "[4] Source: azure_databricks_cookbook.pdf, Page: 264 \n",
            " Content: \n",
            "246     Implementing Near-Real-Time Analytics and Building a Modern Data Warehouse  \n",
            "In this chapter, we will learn how to ingest data coming from disparate sources such as \n",
            "Azure Event Hubs, Azure Data Lake Storage Gen2 (ADLS Gen2) storage, and Azure SQL \n",
            "Database, and how this data can be processed together and stored as a data warehouse \n",
            "model with Facts and Dimension Azure Synapse Analytics, and store processed and raw \n",
            "data in Delta lake and in Cosmos DB for near real time analytics. We will learn how to \n",
            "create a basic data visualization in Power BI and in Azure Databricks on the data that is in \n",
            "Delta Lake and create a near-real-time dashboard for the streaming data that is ingested in \n",
            "Azure Databricks.\n",
            "By the end of this chapter, you will have learned how you can use Delta Lake and adopt \n",
            "Lakehouse patterns in your big data projects.\n",
            "We're going to cover the following recipes in this chapter:\n",
            "• Understanding the scenario for an end-to-end (E2E) solution \n",
            "• Creating required Azure resources for the E2E demonstration\n",
            "• Simulating a workload for streaming data \n",
            "• Processing streaming and batch data using Structured Streaming\n",
            "\n",
            "\n",
            "\n",
            "[5] Source: azure_databricks_cookbook.pdf, Page: 14 \n",
            " Content: \n",
            "xiv     Preface\n",
            "What this book covers\n",
            "Chapter 1, Creating an Azure Databricks Service, explains how to create an Azure \n",
            "Databricks service from the Azure portal, Azure CLI, and ARM templates. Y ou will get to \n",
            "understand the different types of clusters in Azure Databricks, while also learning how  \n",
            "to add users and groups and how to authenticate to Azure Databricks using a personal \n",
            "access token.\n",
            "Chapter 2, Reading and Writing Data from and to Various Azure Services and File Formats, \n",
            "explains how to read and write data from and to various data sources, including Azure \n",
            "SQL DB, Azure Synapse Analytics, ADLS Gen2, Storage Blob, Azure Cosmos, CSV , JSON, \n",
            "and Parquet formats. Y ou will be using native connectors for Azure SQL and an Azure \n",
            "Synapse dedicated pool to read and write the data for improved performance.\n",
            "Chapter 3, Understanding Spark Query Execution, dives deep into query execution and \n",
            "explains how to check the Spark execution plan and learn more about input, shuffle, \n",
            "and output partitions. Y ou will get to understand the different types of joins while \n",
            "working with data frames. Also, you will learn about a few commonly used session-level\n",
            "\n",
            "\n",
            "\n",
            "[6] Source: azure_databricks_cookbook.pdf, Page: 3 \n",
            " Content: \n",
            "Contributors\n",
            "About the authors\n",
            "Phani Raj is an Azure data architect at Microsoft. He has more than 12 years of IT \n",
            "experience and works primarily on the architecture, design, and development of complex \n",
            "data warehouses, OLTP , and big data solutions on Azure for customers across the globe.\n",
            "Vinod Jaiswal is a data engineer at Microsoft. He has more than 13 years of IT experience \n",
            "and works primarily on the architecture, design, and development of complex data \n",
            "warehouses, OLTP , and big data solutions on Azure using Azure data services for a variety \n",
            "of customers. He has also worked on designing and developing real-time data processing \n",
            "and analytics reports from the data ingested from streaming systems using Azure \n",
            "Databricks.\n",
            "\n",
            "\n",
            "\n",
            "[7] Source: azure_databricks_cookbook.pdf, Page: 19 \n",
            " Content: \n",
            "1\n",
            "Creating an Azure \n",
            "Databricks Service\n",
            "Azure Databricks is a high-performance Apache Spark-based platform that has been \n",
            "optimized for the Microsoft Azure cloud. \n",
            "It offers three environments for building and developing data applications:\n",
            "• Databricks Data Science and Engineering: This provides an interactive workspace \n",
            "that enables collaboration between data engineers, data scientists, machine learning \n",
            "engineers, and business analysts and allows you to build big data pipelines. \n",
            "• Databricks SQL: This allows you to run ad hoc SQL queries on your data lake and \n",
            "supports multiple visualization types to explore your query results.\n",
            "• Databricks Machine Learning: Provides end-to-end machine learning \n",
            "environment for feature development, model training , experiment tracking  \n",
            "along with model serving and management.\n",
            "In this chapter, we will cover how to create an Azure Databricks service using the Azure \n",
            "portal, Azure CLI, and ARM templates. We will learn about different types of clusters \n",
            "available in Azure Databricks, how to create jobs, and how to use a personal access token \n",
            "(PAT ) to authenticate Databricks.\n",
            "\n",
            "\n",
            "\n",
            "[8] Source: azure_databricks_cookbook.pdf, Page: 276 \n",
            " Content: \n",
            "258     Implementing Near-Real-Time Analytics and Building a Modern Data Warehouse  \n",
            "We will use this information to create a data warehouse model and create Power BI \n",
            "reports for some interesting insights in upcoming recipes of this chapter. In the next \n",
            "recipe, we will learn how to consume the data from Event Hubs and store the sensor data \n",
            "in different sinks.\n",
            "Processing streaming and batch data using \n",
            "Structured Streaming\n",
            "We tend to see scenarios where we need to process batch data in comma-separated values \n",
            "(CSV) or Parquet format stored in ADLS Gen2 and from real-time streaming sources such \n",
            "as Event Hubs together. In this recipe, we will learn how we use Structured Streaming for \n",
            "both batch and real-time streaming sources and process the data together. We will also fetch \n",
            "the data from Azure SQL Database for all metadata information required for our processing. \n",
            "Getting ready\n",
            "Before starting, you need to have a valid subscription with contributor access, a \n",
            "Databricks workspace (Premium), and an ADLS Gen2 storage account. Also, ensure that \n",
            "you have been through the previous recipes of this chapter.\n",
            "\n",
            "\n",
            "\n",
            "[9] Source: azure_databricks_cookbook.pdf, Page: 1 \n",
            " Content: \n",
            "Azure Databricks \n",
            "Cookbook\n",
            "Accelerate and scale real-time analytics solutions \n",
            "using the Apache Spark-based analytics service\n",
            "Phani Raj\n",
            "Vinod Jaiswal\n",
            "BIRMINGHAM—MUMBAI\n",
            "\n",
            "\n",
            "\n",
            "[10] Source: azure_databricks_cookbook.pdf, Page: 13 \n",
            " Content: \n",
            "Once the big data solution has been built, you will learn how to deploy notebooks to \n",
            "various environments such as UAT and production. Later on, you will learn security \n",
            "aspects associated with data isolation, where you will learn how to restrict access to the \n",
            "data in ADLS that AAD users can see while reading the data from Azure Databricks. \n",
            "Finally, you will learn how to monitor your Azure Databricks cluster utilization by \n",
            "learning about Ganglia reports.\n",
            "Who this book is for\n",
            "This book is for anyone who wants to learn how to ingest and process data from various \n",
            "Azure sources using Azure Databricks and build a modern data warehouse. This book \n",
            "takes a recipe-based approach to help you learn how to ingest, process, secure, deploy,  \n",
            "and monitor the big data solution you have built on Azure Databricks. Working \n",
            "knowledge of Spark and a familiarity with the basics of Azure should be sufficient to get \n",
            "started on this book.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}