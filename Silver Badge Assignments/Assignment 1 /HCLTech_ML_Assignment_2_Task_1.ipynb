{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      },
      "authorship_tag": "ABX9TyNfgnVGbnvHFjwt9RzGm1R/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratnesh003/HCLTech-Tasks/blob/main/Silver%20Badge%20Assignments/Assignment%201%20/HCLTech_ML_Assignment_2_Task_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1"
      ],
      "metadata": {
        "id": "QRpRxMZCALbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use any other open-source model in place of openai and do the summarization of the pdf document using Lang chain and the concept of RAG. use chromadb vector database for this"
      ],
      "metadata": {
        "id": "IDHcZJqpATXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Dependencies"
      ],
      "metadata": {
        "id": "8xS8YUb8HiUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "  requests \\\n",
        "  opentelemetry-api \\\n",
        "  opentelemetry-sdk \\\n",
        "  opentelemetry-proto \\\n",
        "  opentelemetry-exporter-otlp-proto-common \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  langchain-huggingface \\\n",
        "  chromadb \\\n",
        "  pypdf \\\n",
        "  sentence-transformers \\\n",
        "  transformers \\\n",
        "  accelerate \\\n",
        "  bitsandbytes"
      ],
      "metadata": {
        "id": "D74KYu_25C_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the PDF"
      ],
      "metadata": {
        "id": "LXnyNhtnPIhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# Prompt user to upload a PDF\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file name (assumes one PDF)\n",
        "pdf_path = next(iter(uploaded))\n",
        "\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "print(f\"Loaded PDF: {pdf_path}\")\n",
        "print(f\"Total pages loaded: {len(documents)}\")"
      ],
      "metadata": {
        "id": "Cp1UfmSk5Ffa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create the chunks"
      ],
      "metadata": {
        "id": "6Sw1mXuRPSpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200,          # Large enough for semantic coherence\n",
        "    chunk_overlap=300,        # Prevents abrupt cut-offs\n",
        "    separators=[\n",
        "        \"\\n\\n\",               # Paragraphs\n",
        "        \"\\n\",                 # Lines\n",
        "        \".\",                  # Sentences\n",
        "        \" \",                  # Words\n",
        "        \"\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total chunks created: {len(split_docs)}\")\n"
      ],
      "metadata": {
        "id": "Ftg06vun57lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the embedding model"
      ],
      "metadata": {
        "id": "b7EzIyK4PXUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "CKS4udOZ59yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the chromaDB"
      ],
      "metadata": {
        "id": "ejhxhPX3Pd8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=split_docs,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"/content/chroma_db\"\n",
        ")\n",
        "\n",
        "vectorstore.persist()\n"
      ],
      "metadata": {
        "id": "fAqw6D8K5_yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Mistral model"
      ],
      "metadata": {
        "id": "u0Bgao7NPimJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "v5pfAPe16B1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the text generation pipeline"
      ],
      "metadata": {
        "id": "zbgQePQxPqdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n"
      ],
      "metadata": {
        "id": "-0IRAAsJ6Ecn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the retrieval configurations"
      ],
      "metadata": {
        "id": "AJft91KSP04u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 4}\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    return_source_documents=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "e6LMadph6GeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Query and prompt for the generation"
      ],
      "metadata": {
        "id": "yw8AGppYP-Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask user for a custom query\n",
        "query = input(\n",
        "    \"Enter your question about the uploaded PDF:\\n\"\n",
        ").strip()\n",
        "\n",
        "if not query:\n",
        "    raise ValueError(\"Query cannot be empty.\")\n",
        "\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are a senior data engineer.\n",
        "\n",
        "Using the context below, answer the question clearly and technically.\n",
        "Do NOT repeat the context.\n",
        "Do NOT include headings.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cVsdDj-O6In_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streamming the text generation"
      ],
      "metadata": {
        "id": "niLqfj_0QKB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "MAX_INPUT_TOKENS = 8192\n",
        "\n",
        "# Create streamer\n",
        "streamer = TextIteratorStreamer(\n",
        "    tokenizer,\n",
        "    skip_prompt=True,\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Tokenize input WITH attention mask\n",
        "inputs = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=MAX_INPUT_TOKENS\n",
        ").to(model.device)\n",
        "\n",
        "# Background generation\n",
        "thread = Thread(\n",
        "    target=model.generate,\n",
        "    kwargs={\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"streamer\": streamer,\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"do_sample\": True,          # REQUIRED for temperature\n",
        "        \"temperature\": 0.3,\n",
        "        \"repetition_penalty\": 1.1,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id\n",
        "    }\n",
        ")\n",
        "\n",
        "thread.start()\n",
        "\n",
        "# Stream answer tokens only\n",
        "print(\"\\nAnswer:\\n\")\n",
        "for token in streamer:\n",
        "    print(token, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "wrP-q5PW6LD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listing the references used for answer generation"
      ],
      "metadata": {
        "id": "7n_EIoUiQOzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nReferences used from the document:\\n\")\n",
        "\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    source = doc.metadata.get(\"source\", \"Unknown source\")\n",
        "    page = doc.metadata.get(\"page\", \"Unknown page\")\n",
        "    print(f\"[{i}] Source: {source}, Page: {page}\")"
      ],
      "metadata": {
        "id": "iVG7Ra15Lw4R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}